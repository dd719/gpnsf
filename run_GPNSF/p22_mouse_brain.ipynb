{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9998934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/linxuangao')  # 确保父目录在 Python 路径中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626e7510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linxuangao/.conda/envs/glx/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import torch\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import scipy\n",
    "\n",
    "from GPNSF.model import GPNSFModel\n",
    "from GPNSF.utils import *\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c6ee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8fd2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/linxuangao/data_gpnsf/P22 mouse brain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ef1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata1 = sc.read_h5ad(f'{data_dir}/P22_mouse_brain_adata_RNA.h5ad')\n",
    "adata2 = sc.read_h5ad(f'{data_dir}/P22_mouse_brain_adata_atac.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e5ada0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9215, 22914) (9215, 121068)\n",
      "22914 121068\n"
     ]
    }
   ],
   "source": [
    "print(adata1.X.shape, adata2.X.shape)  # (N, D1), (N, D2)\n",
    "print(adata1.n_vars, adata2.n_vars)  # D1, D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c37f398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RNA数据预处理\n",
      "==================================================\n",
      "RNA: 过滤后基因数: 13214\n",
      "RNA: 选择 3000 个高变基因\n",
      "\n",
      "==================================================\n",
      "ATAC数据预处理\n",
      "==================================================\n",
      "ATAC: 过滤后peak数: 120400\n",
      "ATAC: 选择 10000 个高变peak\n",
      "\n",
      "预处理完成!\n",
      "RNA矩阵大小: (9215, 3000)\n",
      "ATAC矩阵大小: (9215, 10000)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_multiome_data(adata_rna, adata_atac, \n",
    "                             rna_n_features=3000,\n",
    "                             atac_n_features=10000):\n",
    "    \"\"\"多组学数据预处理流程\"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"RNA数据预处理\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # RNA数据预处理\n",
    "    # 1. 基本质量控制\n",
    "    sc.pp.filter_genes(adata_rna, min_counts=50)\n",
    "    print(f\"RNA: 过滤后基因数: {adata_rna.n_vars}\")\n",
    "    \n",
    "    # 2. 选择高变基因\n",
    "    sc.pp.highly_variable_genes(\n",
    "        adata_rna, \n",
    "        n_top_genes=rna_n_features,\n",
    "        flavor='seurat_v3'\n",
    "    )\n",
    "    adata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()\n",
    "    print(f\"RNA: 选择 {adata_rna.n_vars} 个高变基因\")\n",
    "    \n",
    "    # 3. 标准化\n",
    "    sc.pp.normalize_total(adata_rna, target_sum=1e4)\n",
    "    sc.pp.log1p(adata_rna)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ATAC数据预处理\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ATAC数据预处理\n",
    "    # 1. 基本质量控制\n",
    "    sc.pp.filter_genes(adata_atac, min_cells=50)\n",
    "    print(f\"ATAC: 过滤后peak数: {adata_atac.n_vars}\")\n",
    "    \n",
    "    # 2. 选择高变peak\n",
    "    sc.pp.highly_variable_genes(\n",
    "        adata_atac, \n",
    "        n_top_genes=atac_n_features,\n",
    "        flavor='seurat_v3'\n",
    "    )\n",
    "    adata_atac = adata_atac[:, adata_atac.var['highly_variable']].copy()\n",
    "    print(f\"ATAC: 选择 {adata_atac.n_vars} 个高变peak\")\n",
    "    \n",
    "    # # 3. 标准化（ATAC通常使用TF-IDF）\n",
    "    # from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    \n",
    "    # if scipy.sparse.issparse(adata_atac.X):\n",
    "    #     X = adata_atac.X.toarray()\n",
    "    # else:\n",
    "    #     X = adata_atac.X\n",
    "    \n",
    "    # tfidf = TfidfTransformer()\n",
    "    # adata_atac.X = tfidf.fit_transform(X)\n",
    "    \n",
    "    print(\"\\n预处理完成!\")\n",
    "    print(f\"RNA矩阵大小: {adata_rna.shape}\")\n",
    "    print(f\"ATAC矩阵大小: {adata_atac.shape}\")\n",
    "    \n",
    "    return adata_rna, adata_atac\n",
    "\n",
    "# 使用示例\n",
    "adata1_filtered, adata2_filtered = preprocess_multiome_data(\n",
    "    adata1.copy(), adata2.copy(),\n",
    "    rna_n_features=3000,\n",
    "    atac_n_features=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a5fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = adata1_filtered.X\n",
    "p = X_1.shape[1]\n",
    "X_2 = adata2_filtered.X\n",
    "q = X_2.shape[1]\n",
    "S = adata1.obsm['spatial']\n",
    "\n",
    "del adata1, adata2  # 释放内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d223666",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 16\n",
    "M = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad8d7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted -> S: torch.Size([9215, 2]) X1: torch.Size([9215, 3000]) X2: torch.Size([9215, 10000]) device= cuda\n"
     ]
    }
   ],
   "source": [
    "# 重新加载 GPNSF 并将 numpy/sparse 转为 torch 张量\n",
    "from importlib import reload\n",
    "import GPNSF.model as gp_model\n",
    "reload(gp_model)\n",
    "from GPNSF.model import GPNSFModel\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def to_torch_tensor(x, device, dtype=torch.float32):\n",
    "    if hasattr(x, \"toarray\"):\n",
    "        x = x.toarray()  # scipy.sparse -> ndarray\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return torch.as_tensor(x, dtype=dtype, device=device)\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(device=device, dtype=dtype)\n",
    "    raise TypeError(f\"Unsupported type: {type(x)}\")\n",
    "\n",
    "S_t = to_torch_tensor(S, device, dtype=torch.float32)\n",
    "X1_t = to_torch_tensor(X_1, device, dtype=torch.float32)\n",
    "X2_t = to_torch_tensor(X_2, device, dtype=torch.float32)\n",
    "\n",
    "print('Converted ->', 'S:', S_t.shape, 'X1:', X1_t.shape, 'X2:', X2_t.shape, 'device=', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bd8fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m         elbo_val \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX2_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         kl_val \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_KL_u()\n\u001b[1;32m     46\u001b[0m         loglik1, loglik2 \u001b[38;5;241m=\u001b[39m compute_loglik_terms(model, X1_t, X2_t)\n",
      "File \u001b[0;32m~/GPNSF/model.py:338\u001b[0m, in \u001b[0;36mGPNSFModel.elbo\u001b[0;34m(self, X1, X2)\u001b[0m\n\u001b[1;32m    335\u001b[0m W1, W2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_W1_W2()\n\u001b[1;32m    336\u001b[0m theta1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta1  \u001b[38;5;66;03m# (p,)\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m Hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_H\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (S, n, K)\u001b[39;00m\n\u001b[1;32m    339\u001b[0m Y1_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    340\u001b[0m Y2_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/GPNSF/model.py:256\u001b[0m, in \u001b[0;36mGPNSFModel.sample_H\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m var_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK):\n\u001b[0;32m--> 256\u001b[0m \tmu_k, var_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_h_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \tmu_list\u001b[38;5;241m.\u001b[39mappend(mu_k)\n\u001b[1;32m    258\u001b[0m \tvar_list\u001b[38;5;241m.\u001b[39mappend(var_k)\n",
      "File \u001b[0;32m~/GPNSF/model.py:221\u001b[0m, in \u001b[0;36mGPNSFModel.q_h_params\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    218\u001b[0m \tL_omega \u001b[38;5;241m=\u001b[39m L_omega \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag_embed(torch\u001b[38;5;241m.\u001b[39mdiagonal(L_omega)) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag_embed(diag)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Cholesky of Kuu\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_robust_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKuu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# v = Kuu^{-1} (delta - muZ)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m rhs \u001b[38;5;241m=\u001b[39m (delta_k \u001b[38;5;241m-\u001b[39m mu_Z_k)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (M,1)\u001b[39;00m\n",
      "File \u001b[0;32m~/GPNSF/model.py:196\u001b[0m, in \u001b[0;36mGPNSFModel._robust_cholesky\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    194\u001b[0m diag_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(A)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    195\u001b[0m extra \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(diag_mean, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "# 构建模型与训练循环（周期性打印 loss/ELBO/KL）\n",
    "model = GPNSFModel(S=S_t, p=p, q=q, K=K, M=M, eta=1.0, num_mc_samples=3, likelihood_x2='ber', \n",
    "                   omega_type='chol', kernel_type='matern32').to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loglik_terms(model, X1, X2):\n",
    "    # 与模型内部保持一致的 Monte Carlo 估计，并适配 X2 的两种似然模式\n",
    "    W1, W2 = model.get_W1_W2()\n",
    "    theta1 = model.theta1\n",
    "    Hs = model.sample_H()  # (S, n, K)\n",
    "    loglik1 = []\n",
    "    loglik2 = []\n",
    "    if model.likelihood_x2 == 'nb':\n",
    "        lambda2_nb = model.lambda2_nb\n",
    "    else:\n",
    "        lambda2_ber = model.lambda2_ber\n",
    "        X2_bin = (X2 > 0).to(X2.dtype)\n",
    "    for t in range(Hs.size(0)):\n",
    "        H = Hs[t]\n",
    "        Y1 = torch.exp(H) @ W1\n",
    "        Y2 = torch.exp(H) @ W2\n",
    "        loglik1.append(nb_log_prob(X1, Y1, theta1).sum())\n",
    "        if model.likelihood_x2 == 'nb':\n",
    "            loglik2.append(nb_log_prob(X2, Y2, lambda2_nb).sum())\n",
    "        else:\n",
    "            p2 = torch.clamp(Y2 * lambda2_ber, min=1e-8, max=1.0 - 1e-8)\n",
    "            loglik2.append(bernoulli_log_prob(X2_bin, p2).sum())\n",
    "    loglik1 = torch.stack(loglik1).mean()\n",
    "    loglik2 = torch.stack(loglik2).mean()\n",
    "    return loglik1, loglik2\n",
    "\n",
    "num_steps = 2000\n",
    "print_every = 100\n",
    "for step in range(1, num_steps + 1):\n",
    "    opt.zero_grad()\n",
    "    loss = model(X1_t, X2_t)  # 负ELBO\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if step == 1 or step % print_every == 0:\n",
    "        with torch.no_grad():\n",
    "            elbo_val = model.elbo(X1_t, X2_t)\n",
    "            kl_val = model.compute_KL_u()\n",
    "            loglik1, loglik2 = compute_loglik_terms(model, X1_t, X2_t)\n",
    "            print(f\"[step {step:03d}] loss={loss.item():.3f}  ELBO={elbo_val.item():.3f}  KL={kl_val.item():.3f}  \"\n",
    "                  f\"loglik1={loglik1.item():.3f}  loglik2={loglik2.item():.3f}\")\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df777806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （保留空白单元，已移除诊断相关内容）\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
